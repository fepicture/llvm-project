; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --force-update
; NOTE: Check cm.push/cm.pop.
; RUN: llc -mtriple=riscv32 -mattr=+experimental-zcmp -verify-machineinstrs < %s \
; RUN: | FileCheck %s -check-prefixes=RV32IZCMP
; RUN: llc -mtriple=riscv64 -mattr=+experimental-zcmp -verify-machineinstrs < %s \
; RUN: | FileCheck %s -check-prefixes=RV64IZCMP
; RUN: llc -mtriple=riscv32 -verify-machineinstrs < %s \
; RUN: | FileCheck -check-prefixes=RV32I %s
; RUN: llc -mtriple=riscv64 -verify-machineinstrs < %s \
; RUN: | FileCheck -check-prefixes=RV64I %s

declare void @test(i8*)

; Function Attrs: optnone
define i32 @foo() {
; RV32IZCMP-LABEL: foo:
; RV32IZCMP:       # %bb.0:
; RV32IZCMP-NEXT:    cm.push {ra}, -64
; RV32IZCMP-NEXT:    .cfi_def_cfa_offset 528
; RV32IZCMP-NEXT:    addi sp, sp, -464
; RV32IZCMP-NEXT:    .cfi_offset ra, -4
; RV32IZCMP-NEXT:    addi a0, sp, 12
; RV32IZCMP-NEXT:    call test@plt
; RV32IZCMP-NEXT:    addi sp, sp, 464
; RV32IZCMP-NEXT:    cm.popretz {ra}, 64
;
; RV64IZCMP-LABEL: foo:
; RV64IZCMP:       # %bb.0:
; RV64IZCMP-NEXT:    cm.push {ra}, -64
; RV64IZCMP-NEXT:    .cfi_def_cfa_offset 528
; RV64IZCMP-NEXT:    addi sp, sp, -464
; RV64IZCMP-NEXT:    .cfi_offset ra, -8
; RV64IZCMP-NEXT:    addi a0, sp, 8
; RV64IZCMP-NEXT:    call test@plt
; RV64IZCMP-NEXT:    addi sp, sp, 464
; RV64IZCMP-NEXT:    cm.popretz {ra}, 64
;
; RV32I-LABEL: foo:
; RV32I:       # %bb.0:
; RV32I-NEXT:    addi sp, sp, -528
; RV32I-NEXT:    .cfi_def_cfa_offset 528
; RV32I-NEXT:    sw ra, 524(sp) # 4-byte Folded Spill
; RV32I-NEXT:    .cfi_offset ra, -4
; RV32I-NEXT:    addi a0, sp, 12
; RV32I-NEXT:    call test@plt
; RV32I-NEXT:    li a0, 0
; RV32I-NEXT:    lw ra, 524(sp) # 4-byte Folded Reload
; RV32I-NEXT:    addi sp, sp, 528
; RV32I-NEXT:    ret
;
; RV64I-LABEL: foo:
; RV64I:       # %bb.0:
; RV64I-NEXT:    addi sp, sp, -528
; RV64I-NEXT:    .cfi_def_cfa_offset 528
; RV64I-NEXT:    sd ra, 520(sp) # 8-byte Folded Spill
; RV64I-NEXT:    .cfi_offset ra, -8
; RV64I-NEXT:    addi a0, sp, 8
; RV64I-NEXT:    call test@plt
; RV64I-NEXT:    li a0, 0
; RV64I-NEXT:    ld ra, 520(sp) # 8-byte Folded Reload
; RV64I-NEXT:    addi sp, sp, 528
; RV64I-NEXT:    ret
  %1 = alloca [512 x i8]
  %2 = getelementptr [512 x i8], [512 x i8]* %1, i32 0, i32 0
  call void @test(i8* %2)
  ret i32 0
}

define dso_local i32 @pushpopret0(i32 signext %size) local_unnamed_addr #0 {
; RV32IZCMP-LABEL: pushpopret0:
; RV32IZCMP:       # %bb.0: # %entry
; RV32IZCMP-NEXT:    .cfi_def_cfa_offset 16
; RV32IZCMP-NEXT:    cm.push {ra, s0}, -16
; RV32IZCMP-NEXT:    .cfi_offset ra, -4
; RV32IZCMP-NEXT:    .cfi_offset s0, -8
; RV32IZCMP-NEXT:    addi s0, sp, 16
; RV32IZCMP-NEXT:    .cfi_def_cfa s0, 0
; RV32IZCMP-NEXT:    addi a0, a0, 15
; RV32IZCMP-NEXT:    andi a0, a0, -16
; RV32IZCMP-NEXT:    sub a0, sp, a0
; RV32IZCMP-NEXT:    mv sp, a0
; RV32IZCMP-NEXT:    call callee_void@plt
; RV32IZCMP-NEXT:    addi sp, s0, -16
; RV32IZCMP-NEXT:    cm.popretz {ra, s0}, 16
;
; RV64IZCMP-LABEL: pushpopret0:
; RV64IZCMP:       # %bb.0: # %entry
; RV64IZCMP-NEXT:    .cfi_def_cfa_offset 16
; RV64IZCMP-NEXT:    cm.push {ra, s0}, -16
; RV64IZCMP-NEXT:    .cfi_offset ra, -8
; RV64IZCMP-NEXT:    .cfi_offset s0, -16
; RV64IZCMP-NEXT:    addi s0, sp, 16
; RV64IZCMP-NEXT:    .cfi_def_cfa s0, 0
; RV64IZCMP-NEXT:    slli a0, a0, 32
; RV64IZCMP-NEXT:    srli a0, a0, 32
; RV64IZCMP-NEXT:    addi a0, a0, 15
; RV64IZCMP-NEXT:    andi a0, a0, -16
; RV64IZCMP-NEXT:    sub a0, sp, a0
; RV64IZCMP-NEXT:    mv sp, a0
; RV64IZCMP-NEXT:    call callee_void@plt
; RV64IZCMP-NEXT:    addi sp, s0, -16
; RV64IZCMP-NEXT:    cm.popretz {ra, s0}, 16
;
; RV32I-LABEL: pushpopret0:
; RV32I:       # %bb.0: # %entry
; RV32I-NEXT:    addi sp, sp, -16
; RV32I-NEXT:    .cfi_def_cfa_offset 16
; RV32I-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32I-NEXT:    sw s0, 8(sp) # 4-byte Folded Spill
; RV32I-NEXT:    .cfi_offset ra, -4
; RV32I-NEXT:    .cfi_offset s0, -8
; RV32I-NEXT:    addi s0, sp, 16
; RV32I-NEXT:    .cfi_def_cfa s0, 0
; RV32I-NEXT:    addi a0, a0, 15
; RV32I-NEXT:    andi a0, a0, -16
; RV32I-NEXT:    sub a0, sp, a0
; RV32I-NEXT:    mv sp, a0
; RV32I-NEXT:    call callee_void@plt
; RV32I-NEXT:    li a0, 0
; RV32I-NEXT:    addi sp, s0, -16
; RV32I-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32I-NEXT:    lw s0, 8(sp) # 4-byte Folded Reload
; RV32I-NEXT:    addi sp, sp, 16
; RV32I-NEXT:    ret
;
; RV64I-LABEL: pushpopret0:
; RV64I:       # %bb.0: # %entry
; RV64I-NEXT:    addi sp, sp, -16
; RV64I-NEXT:    .cfi_def_cfa_offset 16
; RV64I-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64I-NEXT:    sd s0, 0(sp) # 8-byte Folded Spill
; RV64I-NEXT:    .cfi_offset ra, -8
; RV64I-NEXT:    .cfi_offset s0, -16
; RV64I-NEXT:    addi s0, sp, 16
; RV64I-NEXT:    .cfi_def_cfa s0, 0
; RV64I-NEXT:    slli a0, a0, 32
; RV64I-NEXT:    srli a0, a0, 32
; RV64I-NEXT:    addi a0, a0, 15
; RV64I-NEXT:    andi a0, a0, -16
; RV64I-NEXT:    sub a0, sp, a0
; RV64I-NEXT:    mv sp, a0
; RV64I-NEXT:    call callee_void@plt
; RV64I-NEXT:    li a0, 0
; RV64I-NEXT:    addi sp, s0, -16
; RV64I-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64I-NEXT:    ld s0, 0(sp) # 8-byte Folded Reload
; RV64I-NEXT:    addi sp, sp, 16
; RV64I-NEXT:    ret
entry:
  %0 = alloca i8, i32 %size, align 16
  call void @callee_void(i8* nonnull %0)
  ret i32 0
}

define dso_local i32 @pushpopret1(i32 signext %size) local_unnamed_addr #0 {
; RV32IZCMP-LABEL: pushpopret1:
; RV32IZCMP:       # %bb.0: # %entry
; RV32IZCMP-NEXT:    .cfi_def_cfa_offset 16
; RV32IZCMP-NEXT:    cm.push {ra, s0}, -16
; RV32IZCMP-NEXT:    .cfi_offset ra, -4
; RV32IZCMP-NEXT:    .cfi_offset s0, -8
; RV32IZCMP-NEXT:    addi s0, sp, 16
; RV32IZCMP-NEXT:    .cfi_def_cfa s0, 0
; RV32IZCMP-NEXT:    addi a0, a0, 15
; RV32IZCMP-NEXT:    andi a0, a0, -16
; RV32IZCMP-NEXT:    sub a0, sp, a0
; RV32IZCMP-NEXT:    mv sp, a0
; RV32IZCMP-NEXT:    call callee_void@plt
; RV32IZCMP-NEXT:    li a0, 1
; RV32IZCMP-NEXT:    addi sp, s0, -16
; RV32IZCMP-NEXT:    cm.popret {ra, s0}, 16
;
; RV64IZCMP-LABEL: pushpopret1:
; RV64IZCMP:       # %bb.0: # %entry
; RV64IZCMP-NEXT:    .cfi_def_cfa_offset 16
; RV64IZCMP-NEXT:    cm.push {ra, s0}, -16
; RV64IZCMP-NEXT:    .cfi_offset ra, -8
; RV64IZCMP-NEXT:    .cfi_offset s0, -16
; RV64IZCMP-NEXT:    addi s0, sp, 16
; RV64IZCMP-NEXT:    .cfi_def_cfa s0, 0
; RV64IZCMP-NEXT:    slli a0, a0, 32
; RV64IZCMP-NEXT:    srli a0, a0, 32
; RV64IZCMP-NEXT:    addi a0, a0, 15
; RV64IZCMP-NEXT:    andi a0, a0, -16
; RV64IZCMP-NEXT:    sub a0, sp, a0
; RV64IZCMP-NEXT:    mv sp, a0
; RV64IZCMP-NEXT:    call callee_void@plt
; RV64IZCMP-NEXT:    li a0, 1
; RV64IZCMP-NEXT:    addi sp, s0, -16
; RV64IZCMP-NEXT:    cm.popret {ra, s0}, 16
;
; RV32I-LABEL: pushpopret1:
; RV32I:       # %bb.0: # %entry
; RV32I-NEXT:    addi sp, sp, -16
; RV32I-NEXT:    .cfi_def_cfa_offset 16
; RV32I-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32I-NEXT:    sw s0, 8(sp) # 4-byte Folded Spill
; RV32I-NEXT:    .cfi_offset ra, -4
; RV32I-NEXT:    .cfi_offset s0, -8
; RV32I-NEXT:    addi s0, sp, 16
; RV32I-NEXT:    .cfi_def_cfa s0, 0
; RV32I-NEXT:    addi a0, a0, 15
; RV32I-NEXT:    andi a0, a0, -16
; RV32I-NEXT:    sub a0, sp, a0
; RV32I-NEXT:    mv sp, a0
; RV32I-NEXT:    call callee_void@plt
; RV32I-NEXT:    li a0, 1
; RV32I-NEXT:    addi sp, s0, -16
; RV32I-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32I-NEXT:    lw s0, 8(sp) # 4-byte Folded Reload
; RV32I-NEXT:    addi sp, sp, 16
; RV32I-NEXT:    ret
;
; RV64I-LABEL: pushpopret1:
; RV64I:       # %bb.0: # %entry
; RV64I-NEXT:    addi sp, sp, -16
; RV64I-NEXT:    .cfi_def_cfa_offset 16
; RV64I-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64I-NEXT:    sd s0, 0(sp) # 8-byte Folded Spill
; RV64I-NEXT:    .cfi_offset ra, -8
; RV64I-NEXT:    .cfi_offset s0, -16
; RV64I-NEXT:    addi s0, sp, 16
; RV64I-NEXT:    .cfi_def_cfa s0, 0
; RV64I-NEXT:    slli a0, a0, 32
; RV64I-NEXT:    srli a0, a0, 32
; RV64I-NEXT:    addi a0, a0, 15
; RV64I-NEXT:    andi a0, a0, -16
; RV64I-NEXT:    sub a0, sp, a0
; RV64I-NEXT:    mv sp, a0
; RV64I-NEXT:    call callee_void@plt
; RV64I-NEXT:    li a0, 1
; RV64I-NEXT:    addi sp, s0, -16
; RV64I-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64I-NEXT:    ld s0, 0(sp) # 8-byte Folded Reload
; RV64I-NEXT:    addi sp, sp, 16
; RV64I-NEXT:    ret
entry:
  %0 = alloca i8, i32 %size, align 16
  call void @callee_void(i8* nonnull %0)
  ret i32 1
}

define dso_local i32 @pushpopretneg1(i32 signext %size) local_unnamed_addr #0 {
; RV32IZCMP-LABEL: pushpopretneg1:
; RV32IZCMP:       # %bb.0: # %entry
; RV32IZCMP-NEXT:    .cfi_def_cfa_offset 16
; RV32IZCMP-NEXT:    cm.push {ra, s0}, -16
; RV32IZCMP-NEXT:    .cfi_offset ra, -4
; RV32IZCMP-NEXT:    .cfi_offset s0, -8
; RV32IZCMP-NEXT:    addi s0, sp, 16
; RV32IZCMP-NEXT:    .cfi_def_cfa s0, 0
; RV32IZCMP-NEXT:    addi a0, a0, 15
; RV32IZCMP-NEXT:    andi a0, a0, -16
; RV32IZCMP-NEXT:    sub a0, sp, a0
; RV32IZCMP-NEXT:    mv sp, a0
; RV32IZCMP-NEXT:    call callee_void@plt
; RV32IZCMP-NEXT:    li a0, -1
; RV32IZCMP-NEXT:    addi sp, s0, -16
; RV32IZCMP-NEXT:    cm.popret {ra, s0}, 16
;
; RV64IZCMP-LABEL: pushpopretneg1:
; RV64IZCMP:       # %bb.0: # %entry
; RV64IZCMP-NEXT:    .cfi_def_cfa_offset 16
; RV64IZCMP-NEXT:    cm.push {ra, s0}, -16
; RV64IZCMP-NEXT:    .cfi_offset ra, -8
; RV64IZCMP-NEXT:    .cfi_offset s0, -16
; RV64IZCMP-NEXT:    addi s0, sp, 16
; RV64IZCMP-NEXT:    .cfi_def_cfa s0, 0
; RV64IZCMP-NEXT:    slli a0, a0, 32
; RV64IZCMP-NEXT:    srli a0, a0, 32
; RV64IZCMP-NEXT:    addi a0, a0, 15
; RV64IZCMP-NEXT:    andi a0, a0, -16
; RV64IZCMP-NEXT:    sub a0, sp, a0
; RV64IZCMP-NEXT:    mv sp, a0
; RV64IZCMP-NEXT:    call callee_void@plt
; RV64IZCMP-NEXT:    li a0, -1
; RV64IZCMP-NEXT:    addi sp, s0, -16
; RV64IZCMP-NEXT:    cm.popret {ra, s0}, 16
;
; RV32I-LABEL: pushpopretneg1:
; RV32I:       # %bb.0: # %entry
; RV32I-NEXT:    addi sp, sp, -16
; RV32I-NEXT:    .cfi_def_cfa_offset 16
; RV32I-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32I-NEXT:    sw s0, 8(sp) # 4-byte Folded Spill
; RV32I-NEXT:    .cfi_offset ra, -4
; RV32I-NEXT:    .cfi_offset s0, -8
; RV32I-NEXT:    addi s0, sp, 16
; RV32I-NEXT:    .cfi_def_cfa s0, 0
; RV32I-NEXT:    addi a0, a0, 15
; RV32I-NEXT:    andi a0, a0, -16
; RV32I-NEXT:    sub a0, sp, a0
; RV32I-NEXT:    mv sp, a0
; RV32I-NEXT:    call callee_void@plt
; RV32I-NEXT:    li a0, -1
; RV32I-NEXT:    addi sp, s0, -16
; RV32I-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32I-NEXT:    lw s0, 8(sp) # 4-byte Folded Reload
; RV32I-NEXT:    addi sp, sp, 16
; RV32I-NEXT:    ret
;
; RV64I-LABEL: pushpopretneg1:
; RV64I:       # %bb.0: # %entry
; RV64I-NEXT:    addi sp, sp, -16
; RV64I-NEXT:    .cfi_def_cfa_offset 16
; RV64I-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64I-NEXT:    sd s0, 0(sp) # 8-byte Folded Spill
; RV64I-NEXT:    .cfi_offset ra, -8
; RV64I-NEXT:    .cfi_offset s0, -16
; RV64I-NEXT:    addi s0, sp, 16
; RV64I-NEXT:    .cfi_def_cfa s0, 0
; RV64I-NEXT:    slli a0, a0, 32
; RV64I-NEXT:    srli a0, a0, 32
; RV64I-NEXT:    addi a0, a0, 15
; RV64I-NEXT:    andi a0, a0, -16
; RV64I-NEXT:    sub a0, sp, a0
; RV64I-NEXT:    mv sp, a0
; RV64I-NEXT:    call callee_void@plt
; RV64I-NEXT:    li a0, -1
; RV64I-NEXT:    addi sp, s0, -16
; RV64I-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64I-NEXT:    ld s0, 0(sp) # 8-byte Folded Reload
; RV64I-NEXT:    addi sp, sp, 16
; RV64I-NEXT:    ret
entry:
  %0 = alloca i8, i32 %size, align 16
  call void @callee_void(i8* nonnull %0)
  ret i32 -1
}

define dso_local i32 @pushpopret2(i32 signext %size) local_unnamed_addr #0 {
; RV32IZCMP-LABEL: pushpopret2:
; RV32IZCMP:       # %bb.0: # %entry
; RV32IZCMP-NEXT:    .cfi_def_cfa_offset 16
; RV32IZCMP-NEXT:    cm.push {ra, s0}, -16
; RV32IZCMP-NEXT:    .cfi_offset ra, -4
; RV32IZCMP-NEXT:    .cfi_offset s0, -8
; RV32IZCMP-NEXT:    addi s0, sp, 16
; RV32IZCMP-NEXT:    .cfi_def_cfa s0, 0
; RV32IZCMP-NEXT:    addi a0, a0, 15
; RV32IZCMP-NEXT:    andi a0, a0, -16
; RV32IZCMP-NEXT:    sub a0, sp, a0
; RV32IZCMP-NEXT:    mv sp, a0
; RV32IZCMP-NEXT:    call callee_void@plt
; RV32IZCMP-NEXT:    li a0, 2
; RV32IZCMP-NEXT:    addi sp, s0, -16
; RV32IZCMP-NEXT:    cm.popret {ra, s0}, 16
;
; RV64IZCMP-LABEL: pushpopret2:
; RV64IZCMP:       # %bb.0: # %entry
; RV64IZCMP-NEXT:    .cfi_def_cfa_offset 16
; RV64IZCMP-NEXT:    cm.push {ra, s0}, -16
; RV64IZCMP-NEXT:    .cfi_offset ra, -8
; RV64IZCMP-NEXT:    .cfi_offset s0, -16
; RV64IZCMP-NEXT:    addi s0, sp, 16
; RV64IZCMP-NEXT:    .cfi_def_cfa s0, 0
; RV64IZCMP-NEXT:    slli a0, a0, 32
; RV64IZCMP-NEXT:    srli a0, a0, 32
; RV64IZCMP-NEXT:    addi a0, a0, 15
; RV64IZCMP-NEXT:    andi a0, a0, -16
; RV64IZCMP-NEXT:    sub a0, sp, a0
; RV64IZCMP-NEXT:    mv sp, a0
; RV64IZCMP-NEXT:    call callee_void@plt
; RV64IZCMP-NEXT:    li a0, 2
; RV64IZCMP-NEXT:    addi sp, s0, -16
; RV64IZCMP-NEXT:    cm.popret {ra, s0}, 16
;
; RV32I-LABEL: pushpopret2:
; RV32I:       # %bb.0: # %entry
; RV32I-NEXT:    addi sp, sp, -16
; RV32I-NEXT:    .cfi_def_cfa_offset 16
; RV32I-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32I-NEXT:    sw s0, 8(sp) # 4-byte Folded Spill
; RV32I-NEXT:    .cfi_offset ra, -4
; RV32I-NEXT:    .cfi_offset s0, -8
; RV32I-NEXT:    addi s0, sp, 16
; RV32I-NEXT:    .cfi_def_cfa s0, 0
; RV32I-NEXT:    addi a0, a0, 15
; RV32I-NEXT:    andi a0, a0, -16
; RV32I-NEXT:    sub a0, sp, a0
; RV32I-NEXT:    mv sp, a0
; RV32I-NEXT:    call callee_void@plt
; RV32I-NEXT:    li a0, 2
; RV32I-NEXT:    addi sp, s0, -16
; RV32I-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32I-NEXT:    lw s0, 8(sp) # 4-byte Folded Reload
; RV32I-NEXT:    addi sp, sp, 16
; RV32I-NEXT:    ret
;
; RV64I-LABEL: pushpopret2:
; RV64I:       # %bb.0: # %entry
; RV64I-NEXT:    addi sp, sp, -16
; RV64I-NEXT:    .cfi_def_cfa_offset 16
; RV64I-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64I-NEXT:    sd s0, 0(sp) # 8-byte Folded Spill
; RV64I-NEXT:    .cfi_offset ra, -8
; RV64I-NEXT:    .cfi_offset s0, -16
; RV64I-NEXT:    addi s0, sp, 16
; RV64I-NEXT:    .cfi_def_cfa s0, 0
; RV64I-NEXT:    slli a0, a0, 32
; RV64I-NEXT:    srli a0, a0, 32
; RV64I-NEXT:    addi a0, a0, 15
; RV64I-NEXT:    andi a0, a0, -16
; RV64I-NEXT:    sub a0, sp, a0
; RV64I-NEXT:    mv sp, a0
; RV64I-NEXT:    call callee_void@plt
; RV64I-NEXT:    li a0, 2
; RV64I-NEXT:    addi sp, s0, -16
; RV64I-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64I-NEXT:    ld s0, 0(sp) # 8-byte Folded Reload
; RV64I-NEXT:    addi sp, sp, 16
; RV64I-NEXT:    ret
entry:
  %0 = alloca i8, i32 %size, align 16
  call void @callee_void(i8* nonnull %0)
  ret i32 2
}

define dso_local i32 @tailcall(i32 signext %size) local_unnamed_addr #0 {
; RV32IZCMP-LABEL: tailcall:
; RV32IZCMP:       # %bb.0: # %entry
; RV32IZCMP-NEXT:    .cfi_def_cfa_offset 16
; RV32IZCMP-NEXT:    cm.push {ra, s0}, -16
; RV32IZCMP-NEXT:    .cfi_offset ra, -4
; RV32IZCMP-NEXT:    .cfi_offset s0, -8
; RV32IZCMP-NEXT:    addi s0, sp, 16
; RV32IZCMP-NEXT:    .cfi_def_cfa s0, 0
; RV32IZCMP-NEXT:    addi a0, a0, 15
; RV32IZCMP-NEXT:    andi a0, a0, -16
; RV32IZCMP-NEXT:    sub a0, sp, a0
; RV32IZCMP-NEXT:    mv sp, a0
; RV32IZCMP-NEXT:    addi sp, s0, -16
; RV32IZCMP-NEXT:    cm.pop {ra, s0}, 16
; RV32IZCMP-NEXT:    tail callee@plt
;
; RV64IZCMP-LABEL: tailcall:
; RV64IZCMP:       # %bb.0: # %entry
; RV64IZCMP-NEXT:    .cfi_def_cfa_offset 16
; RV64IZCMP-NEXT:    cm.push {ra, s0}, -16
; RV64IZCMP-NEXT:    .cfi_offset ra, -8
; RV64IZCMP-NEXT:    .cfi_offset s0, -16
; RV64IZCMP-NEXT:    addi s0, sp, 16
; RV64IZCMP-NEXT:    .cfi_def_cfa s0, 0
; RV64IZCMP-NEXT:    slli a0, a0, 32
; RV64IZCMP-NEXT:    srli a0, a0, 32
; RV64IZCMP-NEXT:    addi a0, a0, 15
; RV64IZCMP-NEXT:    andi a0, a0, -16
; RV64IZCMP-NEXT:    sub a0, sp, a0
; RV64IZCMP-NEXT:    mv sp, a0
; RV64IZCMP-NEXT:    addi sp, s0, -16
; RV64IZCMP-NEXT:    cm.pop {ra, s0}, 16
; RV64IZCMP-NEXT:    tail callee@plt
;
; RV32I-LABEL: tailcall:
; RV32I:       # %bb.0: # %entry
; RV32I-NEXT:    addi sp, sp, -16
; RV32I-NEXT:    .cfi_def_cfa_offset 16
; RV32I-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32I-NEXT:    sw s0, 8(sp) # 4-byte Folded Spill
; RV32I-NEXT:    .cfi_offset ra, -4
; RV32I-NEXT:    .cfi_offset s0, -8
; RV32I-NEXT:    addi s0, sp, 16
; RV32I-NEXT:    .cfi_def_cfa s0, 0
; RV32I-NEXT:    addi a0, a0, 15
; RV32I-NEXT:    andi a0, a0, -16
; RV32I-NEXT:    sub a0, sp, a0
; RV32I-NEXT:    mv sp, a0
; RV32I-NEXT:    addi sp, s0, -16
; RV32I-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32I-NEXT:    lw s0, 8(sp) # 4-byte Folded Reload
; RV32I-NEXT:    addi sp, sp, 16
; RV32I-NEXT:    tail callee@plt
;
; RV64I-LABEL: tailcall:
; RV64I:       # %bb.0: # %entry
; RV64I-NEXT:    addi sp, sp, -16
; RV64I-NEXT:    .cfi_def_cfa_offset 16
; RV64I-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64I-NEXT:    sd s0, 0(sp) # 8-byte Folded Spill
; RV64I-NEXT:    .cfi_offset ra, -8
; RV64I-NEXT:    .cfi_offset s0, -16
; RV64I-NEXT:    addi s0, sp, 16
; RV64I-NEXT:    .cfi_def_cfa s0, 0
; RV64I-NEXT:    slli a0, a0, 32
; RV64I-NEXT:    srli a0, a0, 32
; RV64I-NEXT:    addi a0, a0, 15
; RV64I-NEXT:    andi a0, a0, -16
; RV64I-NEXT:    sub a0, sp, a0
; RV64I-NEXT:    mv sp, a0
; RV64I-NEXT:    addi sp, s0, -16
; RV64I-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64I-NEXT:    ld s0, 0(sp) # 8-byte Folded Reload
; RV64I-NEXT:    addi sp, sp, 16
; RV64I-NEXT:    tail callee@plt
entry:
  %0 = alloca i8, i32 %size, align 16
  %1 = tail call i32 @callee(i8* nonnull %0)
  ret i32 %1
}

@var = global [5 x i32] zeroinitializer
define dso_local i32 @nocompress(i32 signext %size) local_unnamed_addr #0 {
; RV32IZCMP-LABEL: nocompress:
; RV32IZCMP:       # %bb.0: # %entry
; RV32IZCMP-NEXT:    .cfi_def_cfa_offset 48
; RV32IZCMP-NEXT:    cm.push {ra, s0-s8}, -48
; RV32IZCMP-NEXT:    .cfi_offset ra, -4
; RV32IZCMP-NEXT:    .cfi_offset s0, -8
; RV32IZCMP-NEXT:    .cfi_offset s1, -12
; RV32IZCMP-NEXT:    .cfi_offset s2, -16
; RV32IZCMP-NEXT:    .cfi_offset s3, -20
; RV32IZCMP-NEXT:    .cfi_offset s4, -24
; RV32IZCMP-NEXT:    .cfi_offset s5, -28
; RV32IZCMP-NEXT:    .cfi_offset s6, -32
; RV32IZCMP-NEXT:    .cfi_offset s7, -36
; RV32IZCMP-NEXT:    .cfi_offset s8, -40
; RV32IZCMP-NEXT:    addi s0, sp, 48
; RV32IZCMP-NEXT:    .cfi_def_cfa s0, 0
; RV32IZCMP-NEXT:    addi a0, a0, 15
; RV32IZCMP-NEXT:    andi a0, a0, -16
; RV32IZCMP-NEXT:    sub s2, sp, a0
; RV32IZCMP-NEXT:    mv sp, s2
; RV32IZCMP-NEXT:    lui s1, %hi(var)
; RV32IZCMP-NEXT:    lw s3, %lo(var)(s1)
; RV32IZCMP-NEXT:    lw s4, %lo(var+4)(s1)
; RV32IZCMP-NEXT:    lw s5, %lo(var+8)(s1)
; RV32IZCMP-NEXT:    lw s6, %lo(var+12)(s1)
; RV32IZCMP-NEXT:    addi s7, s1, %lo(var)
; RV32IZCMP-NEXT:    lw s8, 16(s7)
; RV32IZCMP-NEXT:    mv a0, s2
; RV32IZCMP-NEXT:    call callee_void@plt
; RV32IZCMP-NEXT:    sw s8, 16(s7)
; RV32IZCMP-NEXT:    sw s6, %lo(var+12)(s1)
; RV32IZCMP-NEXT:    sw s5, %lo(var+8)(s1)
; RV32IZCMP-NEXT:    sw s4, %lo(var+4)(s1)
; RV32IZCMP-NEXT:    sw s3, %lo(var)(s1)
; RV32IZCMP-NEXT:    mv a0, s2
; RV32IZCMP-NEXT:    addi sp, s0, -48
; RV32IZCMP-NEXT:    cm.pop {ra, s0-s8}, 48
; RV32IZCMP-NEXT:    tail callee@plt
;
; RV64IZCMP-LABEL: nocompress:
; RV64IZCMP:       # %bb.0: # %entry
; RV64IZCMP-NEXT:    .cfi_def_cfa_offset 80
; RV64IZCMP-NEXT:    cm.push {ra, s0-s8}, -80
; RV64IZCMP-NEXT:    .cfi_offset ra, -8
; RV64IZCMP-NEXT:    .cfi_offset s0, -16
; RV64IZCMP-NEXT:    .cfi_offset s1, -24
; RV64IZCMP-NEXT:    .cfi_offset s2, -32
; RV64IZCMP-NEXT:    .cfi_offset s3, -40
; RV64IZCMP-NEXT:    .cfi_offset s4, -48
; RV64IZCMP-NEXT:    .cfi_offset s5, -56
; RV64IZCMP-NEXT:    .cfi_offset s6, -64
; RV64IZCMP-NEXT:    .cfi_offset s7, -72
; RV64IZCMP-NEXT:    .cfi_offset s8, -80
; RV64IZCMP-NEXT:    addi s0, sp, 80
; RV64IZCMP-NEXT:    .cfi_def_cfa s0, 0
; RV64IZCMP-NEXT:    slli a0, a0, 32
; RV64IZCMP-NEXT:    srli a0, a0, 32
; RV64IZCMP-NEXT:    addi a0, a0, 15
; RV64IZCMP-NEXT:    andi a0, a0, -16
; RV64IZCMP-NEXT:    sub s2, sp, a0
; RV64IZCMP-NEXT:    mv sp, s2
; RV64IZCMP-NEXT:    lui s1, %hi(var)
; RV64IZCMP-NEXT:    lw s3, %lo(var)(s1)
; RV64IZCMP-NEXT:    lw s4, %lo(var+4)(s1)
; RV64IZCMP-NEXT:    lw s5, %lo(var+8)(s1)
; RV64IZCMP-NEXT:    lw s6, %lo(var+12)(s1)
; RV64IZCMP-NEXT:    addi s7, s1, %lo(var)
; RV64IZCMP-NEXT:    lw s8, 16(s7)
; RV64IZCMP-NEXT:    mv a0, s2
; RV64IZCMP-NEXT:    call callee_void@plt
; RV64IZCMP-NEXT:    sw s8, 16(s7)
; RV64IZCMP-NEXT:    sw s6, %lo(var+12)(s1)
; RV64IZCMP-NEXT:    sw s5, %lo(var+8)(s1)
; RV64IZCMP-NEXT:    sw s4, %lo(var+4)(s1)
; RV64IZCMP-NEXT:    sw s3, %lo(var)(s1)
; RV64IZCMP-NEXT:    mv a0, s2
; RV64IZCMP-NEXT:    addi sp, s0, -80
; RV64IZCMP-NEXT:    cm.pop {ra, s0-s8}, 80
; RV64IZCMP-NEXT:    tail callee@plt
;
; RV32I-LABEL: nocompress:
; RV32I:       # %bb.0: # %entry
; RV32I-NEXT:    addi sp, sp, -48
; RV32I-NEXT:    .cfi_def_cfa_offset 48
; RV32I-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32I-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32I-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32I-NEXT:    sw s2, 32(sp) # 4-byte Folded Spill
; RV32I-NEXT:    sw s3, 28(sp) # 4-byte Folded Spill
; RV32I-NEXT:    sw s4, 24(sp) # 4-byte Folded Spill
; RV32I-NEXT:    sw s5, 20(sp) # 4-byte Folded Spill
; RV32I-NEXT:    sw s6, 16(sp) # 4-byte Folded Spill
; RV32I-NEXT:    sw s7, 12(sp) # 4-byte Folded Spill
; RV32I-NEXT:    sw s8, 8(sp) # 4-byte Folded Spill
; RV32I-NEXT:    .cfi_offset ra, -4
; RV32I-NEXT:    .cfi_offset s0, -8
; RV32I-NEXT:    .cfi_offset s1, -12
; RV32I-NEXT:    .cfi_offset s2, -16
; RV32I-NEXT:    .cfi_offset s3, -20
; RV32I-NEXT:    .cfi_offset s4, -24
; RV32I-NEXT:    .cfi_offset s5, -28
; RV32I-NEXT:    .cfi_offset s6, -32
; RV32I-NEXT:    .cfi_offset s7, -36
; RV32I-NEXT:    .cfi_offset s8, -40
; RV32I-NEXT:    addi s0, sp, 48
; RV32I-NEXT:    .cfi_def_cfa s0, 0
; RV32I-NEXT:    addi a0, a0, 15
; RV32I-NEXT:    andi a0, a0, -16
; RV32I-NEXT:    sub s2, sp, a0
; RV32I-NEXT:    mv sp, s2
; RV32I-NEXT:    lui s1, %hi(var)
; RV32I-NEXT:    lw s3, %lo(var)(s1)
; RV32I-NEXT:    lw s4, %lo(var+4)(s1)
; RV32I-NEXT:    lw s5, %lo(var+8)(s1)
; RV32I-NEXT:    lw s6, %lo(var+12)(s1)
; RV32I-NEXT:    addi s7, s1, %lo(var)
; RV32I-NEXT:    lw s8, 16(s7)
; RV32I-NEXT:    mv a0, s2
; RV32I-NEXT:    call callee_void@plt
; RV32I-NEXT:    sw s8, 16(s7)
; RV32I-NEXT:    sw s6, %lo(var+12)(s1)
; RV32I-NEXT:    sw s5, %lo(var+8)(s1)
; RV32I-NEXT:    sw s4, %lo(var+4)(s1)
; RV32I-NEXT:    sw s3, %lo(var)(s1)
; RV32I-NEXT:    mv a0, s2
; RV32I-NEXT:    addi sp, s0, -48
; RV32I-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32I-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32I-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32I-NEXT:    lw s2, 32(sp) # 4-byte Folded Reload
; RV32I-NEXT:    lw s3, 28(sp) # 4-byte Folded Reload
; RV32I-NEXT:    lw s4, 24(sp) # 4-byte Folded Reload
; RV32I-NEXT:    lw s5, 20(sp) # 4-byte Folded Reload
; RV32I-NEXT:    lw s6, 16(sp) # 4-byte Folded Reload
; RV32I-NEXT:    lw s7, 12(sp) # 4-byte Folded Reload
; RV32I-NEXT:    lw s8, 8(sp) # 4-byte Folded Reload
; RV32I-NEXT:    addi sp, sp, 48
; RV32I-NEXT:    tail callee@plt
;
; RV64I-LABEL: nocompress:
; RV64I:       # %bb.0: # %entry
; RV64I-NEXT:    addi sp, sp, -80
; RV64I-NEXT:    .cfi_def_cfa_offset 80
; RV64I-NEXT:    sd ra, 72(sp) # 8-byte Folded Spill
; RV64I-NEXT:    sd s0, 64(sp) # 8-byte Folded Spill
; RV64I-NEXT:    sd s1, 56(sp) # 8-byte Folded Spill
; RV64I-NEXT:    sd s2, 48(sp) # 8-byte Folded Spill
; RV64I-NEXT:    sd s3, 40(sp) # 8-byte Folded Spill
; RV64I-NEXT:    sd s4, 32(sp) # 8-byte Folded Spill
; RV64I-NEXT:    sd s5, 24(sp) # 8-byte Folded Spill
; RV64I-NEXT:    sd s6, 16(sp) # 8-byte Folded Spill
; RV64I-NEXT:    sd s7, 8(sp) # 8-byte Folded Spill
; RV64I-NEXT:    sd s8, 0(sp) # 8-byte Folded Spill
; RV64I-NEXT:    .cfi_offset ra, -8
; RV64I-NEXT:    .cfi_offset s0, -16
; RV64I-NEXT:    .cfi_offset s1, -24
; RV64I-NEXT:    .cfi_offset s2, -32
; RV64I-NEXT:    .cfi_offset s3, -40
; RV64I-NEXT:    .cfi_offset s4, -48
; RV64I-NEXT:    .cfi_offset s5, -56
; RV64I-NEXT:    .cfi_offset s6, -64
; RV64I-NEXT:    .cfi_offset s7, -72
; RV64I-NEXT:    .cfi_offset s8, -80
; RV64I-NEXT:    addi s0, sp, 80
; RV64I-NEXT:    .cfi_def_cfa s0, 0
; RV64I-NEXT:    slli a0, a0, 32
; RV64I-NEXT:    srli a0, a0, 32
; RV64I-NEXT:    addi a0, a0, 15
; RV64I-NEXT:    andi a0, a0, -16
; RV64I-NEXT:    sub s2, sp, a0
; RV64I-NEXT:    mv sp, s2
; RV64I-NEXT:    lui s1, %hi(var)
; RV64I-NEXT:    lw s3, %lo(var)(s1)
; RV64I-NEXT:    lw s4, %lo(var+4)(s1)
; RV64I-NEXT:    lw s5, %lo(var+8)(s1)
; RV64I-NEXT:    lw s6, %lo(var+12)(s1)
; RV64I-NEXT:    addi s7, s1, %lo(var)
; RV64I-NEXT:    lw s8, 16(s7)
; RV64I-NEXT:    mv a0, s2
; RV64I-NEXT:    call callee_void@plt
; RV64I-NEXT:    sw s8, 16(s7)
; RV64I-NEXT:    sw s6, %lo(var+12)(s1)
; RV64I-NEXT:    sw s5, %lo(var+8)(s1)
; RV64I-NEXT:    sw s4, %lo(var+4)(s1)
; RV64I-NEXT:    sw s3, %lo(var)(s1)
; RV64I-NEXT:    mv a0, s2
; RV64I-NEXT:    addi sp, s0, -80
; RV64I-NEXT:    ld ra, 72(sp) # 8-byte Folded Reload
; RV64I-NEXT:    ld s0, 64(sp) # 8-byte Folded Reload
; RV64I-NEXT:    ld s1, 56(sp) # 8-byte Folded Reload
; RV64I-NEXT:    ld s2, 48(sp) # 8-byte Folded Reload
; RV64I-NEXT:    ld s3, 40(sp) # 8-byte Folded Reload
; RV64I-NEXT:    ld s4, 32(sp) # 8-byte Folded Reload
; RV64I-NEXT:    ld s5, 24(sp) # 8-byte Folded Reload
; RV64I-NEXT:    ld s6, 16(sp) # 8-byte Folded Reload
; RV64I-NEXT:    ld s7, 8(sp) # 8-byte Folded Reload
; RV64I-NEXT:    ld s8, 0(sp) # 8-byte Folded Reload
; RV64I-NEXT:    addi sp, sp, 80
; RV64I-NEXT:    tail callee@plt
entry:
  %0 = alloca i8, i32 %size, align 16
  %val = load [5 x i32], [5 x i32]* @var
  call void @callee_void(i8* nonnull %0)
  store volatile [5 x i32] %val, [5 x i32]* @var
  %1 = tail call i32 @callee(i8* nonnull %0)
  ret i32 %1
}

declare void @callee_void(i8*)
declare i32 @callee(i8*)